{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d93603c",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "All-in-one package for performing basic and advanced natural language processing, with special optimization \"quickstart\" features for certain languages. See [spaCy Language Support](https://spacy.io/usage/models#languages) for details.\n",
    "\n",
    "## Features\n",
    "\n",
    "* **Tokenization**: Segmenting text into individual \"tokens\", that is, words, punctuations marks, numbers, etc.\n",
    "\n",
    "* **Part-of-speech (POS) Tagging**: Assigning grammatical word types to tokens, like \"verb\" or \"noun\" (using [Universal POS Tags](https://universaldependencies.org/u/pos/) with `.pos_` and [Penn Part of Speech Tags](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html) with `.tag_`).\n",
    "\n",
    "* **Dependency Parsing**: Assigning syntactic dependency labels, describing the relations between individual tokens, as in subject, object, dependent clause, etc.\n",
    "\n",
    "* **Lemmatization**: Determining the base form, or *lemma* of a word.  The lemma of \"went\" is \"to go\", and the lemma of \"trees\" is \"tree\".\n",
    "\n",
    "* **Named Entity Recognition (NER)**: Labeling \"real-world\" objects with names, such as persons, companies or locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88e80",
   "metadata": {},
   "source": [
    "## Download and Load Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download language package.\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b20ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load resources.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b6be7",
   "metadata": {},
   "source": [
    "## Some Linguistic Basics\n",
    "\n",
    "The `nlp()` function initiates a [pipeline](https://spacy.io/usage/processing-pipelines) that first tokenizers the text, then runs a series of processors which can be customized. The default English pipeline we loaded above includes a tagger, a lemmatizer, a parser and an entity recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence written by ChatGPT after I asked it to write a sentence with all (16) parts of speech. \n",
    "# It repeatedly failed.\n",
    "\n",
    "sentence = \"Wow! Oh no, I forgot to buy ten oranges and seven apples for the party tomorrow, but I promise I'll get them soon.\"\n",
    "\n",
    "# The nlp() function initiates a pipeline.\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8a43b",
   "metadata": {},
   "source": [
    "The `count_by` function can help count things, but takes a bit of coaxing to reveal helpful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import *\n",
    "pos_counts = doc.count_by(POS)\n",
    "print(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in pos_counts.items():\n",
    "    human_readable_tag = doc.vocab[key].text\n",
    "    print(human_readable_tag, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e56352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da3448",
   "metadata": {},
   "source": [
    "Sentence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our natural resources are developed by an earnest culture of the arts and peace.\"\n",
    "doc2 = nlp(sentence)\n",
    "\n",
    "displacy.render(doc, style=\"dep\")\n",
    "\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\"}\n",
    "# displacy.render(doc2, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd968a0",
   "metadata": {},
   "source": [
    "## Longer Texts\n",
    "\n",
    "Processing is lightning-fast on individual texts, but there are size limitations, and even shorter texts will take a few moments to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/KafkaMetamorphosis.txt\") as f:\n",
    "    text = nlp(f.read())\n",
    "print(text[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad53a57",
   "metadata": {},
   "source": [
    "Sentence-level segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in text.sents:\n",
    "    print(sent)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for sent in text.sents:\n",
    "    print(count, sent.text.strip())\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = list(text.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33deeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list[387]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac71f2",
   "metadata": {},
   "source": [
    "## Multi-Token Segments\n",
    "\n",
    "spaCy doesn't place much emphasis on \"bigrams\" or \"trigrams\" as some other text analysis packages do. Instead it offers \"noun chunks\" which are single- or multi-word phrases derived from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in text.noun_chunks:\n",
    "    print(\" -- \".join([chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd6a27",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "\n",
    "String together sequences of tokens with a variety of characteristics to highlight interesting linguistic occasions within the text\n",
    "\n",
    "* [available token attributes](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes): A list of all the ways tokens can be matched.\n",
    "* [match tester](https://demos.explosion.ai/matcher): A user interface for testing out your matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beef2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [\n",
    "    [\n",
    "     {\"POS\": \"ADJ\", \"OP\": \"?\"}, \n",
    "     {\"POS\": \"ADJ\"}, \n",
    "     {\"POS\": \"NOUN\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "pattern2 = [\n",
    "    [{\"LENGTH\": {\">=\": 16}}]\n",
    "]\n",
    "\n",
    "pattern3 = [\n",
    "    [{\"POS\": \"ADJ\"}, {\"LOWER\": {\"IN\": [\"legs\", \"thorax\", \"head\", \"abdomen\", \"back\", \"eyes\", \"mouth\", \"antennae\"]}}]\n",
    "]\n",
    "\n",
    "pattern4 = [\n",
    "    [{\"ENT_TYPE\": \"PERSON\"}, {\"POS\": \"VERB\"}]\n",
    "]\n",
    "\n",
    "\n",
    "matcher.add(\"Adj-Noun\", pattern1)\n",
    "# matcher.add(\"Long-Words\", pattern2)\n",
    "# matcher.add(\"Legs\", pattern3)\n",
    "# matcher.add(\"Person\", pattern4)\n",
    "matches = matcher(text)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab12982",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affddeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in text.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ad0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(text[:500], style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
