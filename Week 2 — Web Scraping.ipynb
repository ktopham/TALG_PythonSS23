{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Web scraping is a technique for harvesting data from a webpage. *Scraping* (and its slightly more complex counterpart, **crawling**) is an effective way to get text data and process it for analysis. \n",
    "\n",
    "There are multiple techniques, but today we're going to use the requests library to gather HTML data and then use BeautifulSoup to process that HTML into the data we want. \n",
    "\n",
    "in this notebook, we will **scrape** data from the White House website -- in particular, we will scrape the transcripts of press briefings related to covid-19. We will scrape from this url: https://www.whitehouse.gov/briefing-room/press-briefings/ \n",
    "We will also learn about libraries, caching, defining functions, and some basics of HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "First, we need to **import** our Libraries. \n",
    "Libraries in python are essentially a chunk of code that we can import and use. They might contain new fuctions or object types that we can use in our code. If you think of programming with python as spellcasting, each library is like a spellbook. When we \"import\" a library, we're telling python to open that spellbook so we can cast its spells (call its functions) at will.\n",
    "\n",
    "Libraries are often structured as a set of modules that you can import separately (see below where we **import** BeautifulSoup **from** bs4). You may decide to import a specific module from a library to help your code run faster, like jumping to a specific chapter in a spellbook. \n",
    "\n",
    "There are many, many, many python libraries available. Good ones will have documentation available, though the quality of that documentation varies.  \n",
    "\n",
    "It is standard practice to import any libraries you will use at the very beginning of your file. You can technically imort a library any time before you need it, but it's helpful to group them all at the biginning so that you (and future re-users of your code) know what libraries need to be installed. (All libraries we'll use today should already be installed in our environment. We won't cover installation today, but we can help if oyu need it!)\n",
    "\n",
    "# What libraries will we use today?\n",
    "\n",
    "## requests\n",
    "Requests allows us to \"request\" data from a data source like an API or a web page.\n",
    "\n",
    "## json\n",
    "This library allows us to read and write JSON files. JSON, which stands for JavaScript Object Notation, is a human and machine-readable data interchange format. In python, it's often used to store or exchange dictionaries and lists for later use. We will use this for caching, which we will explain later.\n",
    "\n",
    "## bs4 & BeautifulSoup\n",
    "BeautifulSoup is a commonly used webscraping tool. It allows us to parse the HTML returned by the requests library. bs4 stands for BeautifulSoup4, the larger library, and BeautifulSoup is a module. \n",
    "\n",
    "## datetime\n",
    "datetime is a standard library that comes with Python, and it allows us to parse dates and times as objects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "## What is Caching?\n",
    "Caching is the  process of storing data so that future requests for that data can be served faster. In our context, it means saving the data we download, first as a python disctionary, then as a JSON file. \n",
    "\n",
    "## Why Cache?\n",
    "Caching is useful for a lot of different reasons:\n",
    "1) Caching makes your code run faster. If we **request** data from an API or website, we will have to wait until all of it is downloaded. Loading our data from a cache is much faster.\n",
    "2) Caching protects you from being blocked by APIs or websites. APIs often have rate limits to prevent abuse. Caching your data means that you won't have to request every time you run your code, resulting in fewer queries, less likely\n",
    "3) Caching keeps a record of your data. You can edit the caching code so that it records the date and time the data was collected, allowing for comparisons.\n",
    "\n",
    "## How Cache?\n",
    "In order to get what we need, the cache needs to 1) be readable by python and 2) remain persistent outside the python script. The technique outlined here saves the cache as a python dictionary, then saves that dictionary as a JSON file that can be read the next time the script is run. \n",
    "The block of code below is pretty much endlessly reusable--just make sure you update your file name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FNAME = 'wh_cache.json' \n",
    "#try to open the cache file\n",
    "try:\n",
    "    cache_file = open(CACHE_FNAME, 'r') #open the file\n",
    "    cache_contents = cache_file.read() # read the file\n",
    "    CACHE_DICTION = json.loads(cache_contents) #load the JSON string into a dictionary called CACHE_DICTION\n",
    "    cache_file.close()\n",
    "#if the file won't open, create an empty dictionary where we will store the results\n",
    "except:\n",
    "    CACHE_DICTION = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*you may notice that the variable CACHE_DICTION and CACHE_FNAME are uppercase rather than lowercase, which goes against what we've described as python convention. That is done because they are **global** variables, which we will need to access within and outside the functions we will define. Making them uppercase prompts the reader to pay attention to these variables throughout the rest of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape one page and break it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own Functions\n",
    "You've been introduced to the pre-made functions in python, like print(), type(), and len(). But we haven't touched one of the most powerful tools in python, **function definition**, which allows us to create customized functions to suit our own needs. \n",
    "\n",
    "## When should I make a function? Why should I bother, when I can just write the code?\n",
    "This may be confusing to new python learners who are just starting out writing a litte bit of code at a time. Once you start writing more complex scritps, however, defining functions can be a really helpful way of organizing your code and making it readable for other programmers (and for future you!). \n",
    "\n",
    "Let's start by making a simple function that turns a string into a list of words. *(Note that python doesn't know what we mean by 'word' here. How do we make it do what we want?)*\n",
    "\n",
    "## Anatomy of a function\n",
    "input: Also known as arguments, this is what you will give to your function to work with.  In our example, the function will take a string as input.\n",
    "process: the \"meat\" of your function. What does it do with the input? You can include conditionals, loops, and try/except blocks. *(It might be helpful to include print statements as you're creating a function to make sure it's doing what you want!)*\n",
    "return: the object that the function returns--for example, if the function is testing whether a string contains the word \"catfish\", it might return a boolean value (True or False); or if the function is calculating word frequency, it might return a dictionary with the word as a key and the number of times it appears as its value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(text):\n",
    "    text_list = text.split()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'on',\n",
       " 'a',\n",
       " 'dreary',\n",
       " 'night',\n",
       " 'of',\n",
       " 'November',\n",
       " 'that',\n",
       " 'I',\n",
       " 'beheld',\n",
       " 'my',\n",
       " 'man',\n",
       " 'completed.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_words(\"It was on a dreary night of November that I beheld my man completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sentence is long!\n"
     ]
    }
   ],
   "source": [
    "frankenstein = split_words(\"It was on a dreary night of November that I beheld my man completed.\")\n",
    "if len(frankenstein) > 5:\n",
    "    print(\"this sentence is long!\")\n",
    "elif len(frankenstein) < 5:\n",
    "    print(\"this sentence is short!\")\n",
    "else:\n",
    "    print(\"I have no idea what's happening!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function that will test whether the word \"catfish\" appears in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_catfish(): #what is the input?\n",
    "#     process: what does the function do?\n",
    "    return #what does the function return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then run this code to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"canning of catfish\"\n",
    "s2 = \"This is my cat! His name is Fish.\"\n",
    "\n",
    "print(is_catfish(s1))\n",
    "print(is_catfish(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you'll find my function to write new data to the cache. Read through it and try to unpack its parts (input, process, return). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_cache(url, req_text):\n",
    "    CACHE_DICTION[url] = req_text\n",
    "    dumped_json_cache = json.dumps(CACHE_DICTION)\n",
    "    fw = open(CACHE_FNAME,\"w\")\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 1) the URL given to requests, and 2) a varilable called \"req_text\", a string containing the results of that request. \n",
    "\n",
    "Process: 1) create a dictionary entry in CACHE_DICTION with the url as the key and req_text as a value. 2) convert CACHE_DICTION to a json string and save it to the cache filename defined earlier. 3) close the file. \n",
    "\n",
    "Return: None! This function doesn't need to return anything, so we set return to None. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our caching functions set, we can get to the fun part: web scraping!\n",
    "\n",
    "But first: let's look at our data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape one page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse with beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling is when you use links found in HTML pages to **crawl** onto those pages and scrape them in turn. You can think of it as request chaining: you scrape a page with a feed for all the links, then scrape each of those pages. This allows us to get a lot of data quickly without having to copy and paste a lot of urls.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
